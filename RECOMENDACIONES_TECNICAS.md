# ğŸ› ï¸ RECOMENDACIONES TÃ‰CNICAS - Stack para ProducciÃ³n

## 1. ğŸ” BACKEND API

### OpciÃ³n A: FastAPI (Python) âœ… RECOMENDADO
**Para:** AnÃ¡lisis estadÃ­sticos avanzados

```python
# main.py
from fastapi import FastAPI, UploadFile, HTTPException, Depends
from fastapi.security import HTTPBearer
from fastapi.responses import JSONResponse
from fastapi_cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

app = FastAPI(title="Relational Insight API")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://your-domain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/api/analyze")
async def analyze_file(
    file: UploadFile,
    current_user: str = Depends(verify_token)
):
    """
    Analizar archivo CSV/XLSX
    
    Rate Limit: 100 req/min por usuario
    Auth: JWT token
    """
    try:
        # Validar archivo
        if not file.filename.endswith(('.csv', '.xlsx')):
            raise HTTPException(400, "Solo CSV/XLSX")
        
        if file.size > 20_000_000:
            raise HTTPException(400, "Archivo > 20MB")
        
        # Procesar
        df = pd.read_csv(file.file) if file.filename.endswith('.csv') else pd.read_excel(file.file)
        
        # Validar datos
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        if len(numeric_cols) < 2:
            raise HTTPException(400, "MÃ­nimo 2 columnas numÃ©ricas")
        
        # Calcular correlaciones
        results = calculate_correlations(df[numeric_cols])
        
        # Guardar en base de datos
        save_analysis(current_user, file.filename, results)
        
        return {
            "success": True,
            "correlation_results": results,
            "numeric_columns": numeric_cols
        }
    
    except Exception as e:
        logger.error(f"Analysis failed: {e}", extra={"user": current_user})
        raise HTTPException(500, "Error during analysis")

def calculate_correlations(df):
    results = []
    cols = df.columns.tolist()
    
    for i, col1 in enumerate(cols):
        for col2 in cols[i+1:]:
            x, y = df[col1].dropna(), df[col2].dropna()
            
            results.append({
                "column_a": col1,
                "column_b": col2,
                "pearson": float(pearsonr(x, y)[0]),
                "spearman": float(spearmanr(x, y)[0]),
                "kendall": float(kendalltau(x, y)[0]),
            })
    
    return results
```

**Ventajas:**
- âœ… Excelente para anÃ¡lisis cientÃ­ficos
- âœ… FÃ¡cil integraciÃ³n con pandas/scipy/numpy
- âœ… Performance superior en cÃ¡lculos numÃ©ricos
- âœ… Gran comunidad

**Stack Completo:**
```
FastAPI + Uvicorn (app)
PostgreSQL + SQLAlchemy (DB)
Redis (cache + rate limiting)
JWT (autenticaciÃ³n)
Pydantic (validaciÃ³n)
```

**Deploy:**
```bash
# Local
uvicorn main:app --reload

# Docker
docker build -t relational-insight-api .
docker run -p 8000:8000 relational-insight-api

# ProducciÃ³n
gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app
```

---

### OpciÃ³n B: Express.js (Node.js)
**Para:** Si prefieres stack 100% JavaScript

```javascript
// server.js
const express = require('express');
const cors = require('cors');
const rateLimit = require('express-rate-limit');
const helmet = require('helmet');
const multer = require('multer');
const ExcelJS = require('exceljs');
const Papa = require('papaparse');

const app = express();

// Seguridad
app.use(helmet());

// CORS
app.use(cors({
  origin: process.env.FRONTEND_URL,
  credentials: true
}));

// Rate Limiting
const limiter = rateLimit({
  windowMs: 60 * 1000, // 1 minuto
  max: 100,
  message: 'Too many requests'
});

app.use('/api/', limiter);

// File Upload
const upload = multer({ 
  storage: multer.memoryStorage(),
  limits: { fileSize: 20 * 1024 * 1024 }
});

// AnÃ¡lisis
app.post('/api/analyze', authenticateToken, upload.single('file'), async (req, res) => {
  try {
    const { file } = req;
    
    // Validar
    if (!file) throw new Error('No file provided');
    if (!['.csv', '.xlsx'].some(ext => file.originalname.endsWith(ext))) {
      throw new Error('Invalid file type');
    }
    
    // Parse
    let rows = [];
    if (file.originalname.endsWith('.csv')) {
      const text = file.buffer.toString('utf-8');
      rows = Papa.parse(text, { header: true }).data;
    } else {
      const workbook = new ExcelJS.Workbook();
      await workbook.xlsx.load(file.buffer);
      const worksheet = workbook.getWorksheet(1);
      rows = worksheet.getSheetValues().slice(1);
    }
    
    // Procesar
    const results = analyzeCorrelations(rows);
    
    res.json({ success: true, ...results });
  } catch (error) {
    logger.error(error);
    res.status(400).json({ error: error.message });
  }
});

// AutenticaciÃ³n
function authenticateToken(req, res, next) {
  const token = req.headers['authorization']?.split(' ')[1];
  if (!token) return res.status(401).json({ error: 'Token required' });
  
  jwt.verify(token, process.env.JWT_SECRET, (err, user) => {
    if (err) return res.status(403).json({ error: 'Invalid token' });
    req.user = user;
    next();
  });
}

app.listen(8000, () => {
  console.log('Server running on port 8000');
});
```

**Ventajas:**
- âœ… Stack unificado JavaScript
- âœ… Buen performance
- âœ… FÃ¡cil de escalar

**Desventajas:**
- âŒ No tan optimizado para operaciones numÃ©ricas

---

## 2. ğŸ—„ï¸ BASE DE DATOS

### PostgreSQL âœ… RECOMENDADO

```sql
-- Usuarios
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- AnÃ¡lisis guardados
CREATE TABLE analyses (
  id SERIAL PRIMARY KEY,
  user_id INTEGER REFERENCES users(id),
  filename VARCHAR(255),
  file_path VARCHAR(512),
  correlation_results JSONB,
  numeric_columns TEXT[],
  created_at TIMESTAMP DEFAULT NOW(),
  is_public BOOLEAN DEFAULT FALSE
);

-- Logs de auditorÃ­a
CREATE TABLE audit_logs (
  id SERIAL PRIMARY KEY,
  user_id INTEGER REFERENCES users(id),
  action VARCHAR(50),
  resource_type VARCHAR(50),
  resource_id INTEGER,
  details JSONB,
  ip_address VARCHAR(45),
  created_at TIMESTAMP DEFAULT NOW()
);

-- Rate limiting
CREATE TABLE rate_limits (
  id SERIAL PRIMARY KEY,
  user_id INTEGER,
  endpoint VARCHAR(255),
  count INTEGER DEFAULT 1,
  reset_at TIMESTAMP,
  UNIQUE(user_id, endpoint)
);

-- Ãndices para performance
CREATE INDEX idx_analyses_user_id ON analyses(user_id);
CREATE INDEX idx_analyses_created_at ON analyses(created_at);
CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_created_at ON audit_logs(created_at);
```

**ORM Recomendado:** SQLAlchemy (Python) o TypeORM (Node.js)

---

## 3. ğŸ” AUTENTICACIÃ“N

### JWT + Refresh Tokens

```typescript
// auth/jwt.ts
import jwt from 'jsonwebtoken';

interface TokenPayload {
  userId: number;
  email: string;
  iat: number;
  exp: number;
}

export function generateTokens(userId: number, email: string) {
  const accessToken = jwt.sign(
    { userId, email },
    process.env.JWT_SECRET!,
    { expiresIn: '15m' }
  );

  const refreshToken = jwt.sign(
    { userId, type: 'refresh' },
    process.env.REFRESH_TOKEN_SECRET!,
    { expiresIn: '7d' }
  );

  return { accessToken, refreshToken };
}

export function verifyToken(token: string): TokenPayload {
  return jwt.verify(token, process.env.JWT_SECRET!) as TokenPayload;
}

export function verifyRefreshToken(token: string) {
  return jwt.verify(token, process.env.REFRESH_TOKEN_SECRET!);
}
```

**Endpoints:**

```
POST   /auth/register       - Crear cuenta
POST   /auth/login          - Login
POST   /auth/refresh        - Refresh token
POST   /auth/logout         - Logout
POST   /auth/forgot-password - Reset password
```

### OAuth2 (Alternativa)

```typescript
// Para Google OAuth
import { OAuth2Client } from 'google-auth-library';

const client = new OAuth2Client(
  process.env.GOOGLE_CLIENT_ID,
  process.env.GOOGLE_CLIENT_SECRET,
  'http://localhost:3000/auth/google/callback'
);

// En backend
app.post('/auth/google', async (req, res) => {
  const { token } = req.body;
  const ticket = await client.verifyIdToken({ idToken: token });
  const { email, picture } = ticket.getPayload();
  
  // Buscar o crear usuario
  const user = await findOrCreateUser({ email, picture });
  
  const { accessToken, refreshToken } = generateTokens(user.id, user.email);
  res.json({ accessToken, refreshToken, user });
});
```

---

## 4. âš¡ CACHING & RATE LIMITING

### Redis

```bash
# Installation
docker run -d -p 6379:6379 redis:latest

# O en cÃ³digo
npm install redis
```

```typescript
// services/cache.ts
import { createClient } from 'redis';

const redisClient = createClient({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379')
});

export async function cacheAnalysis(
  userId: number,
  fileHash: string,
  results: any,
  ttl: number = 3600
) {
  const key = `analysis:${userId}:${fileHash}`;
  await redisClient.setEx(key, ttl, JSON.stringify(results));
}

export async function getCachedAnalysis(userId: number, fileHash: string) {
  const key = `analysis:${userId}:${fileHash}`;
  const cached = await redisClient.get(key);
  return cached ? JSON.parse(cached) : null;
}

// Rate limiting
export async function checkRateLimit(userId: number, endpoint: string) {
  const key = `ratelimit:${userId}:${endpoint}`;
  const count = await redisClient.incr(key);
  
  if (count === 1) {
    await redisClient.expire(key, 60); // 1 minuto
  }
  
  return count <= 100; // 100 requests per minute
}
```

---

## 5. ğŸ“Š MONITORING & LOGGING

### Sentry (Error Tracking)

```typescript
import * as Sentry from "@sentry/nextjs";

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  environment: process.env.NODE_ENV,
  integrations: [
    new Sentry.Replay({
      maskAllText: true,
      blockAllMedia: true,
    }),
  ],
  tracesSampleRate: 1.0,
  replaysSessionSampleRate: 0.1,
  replaysOnErrorSampleRate: 1.0,
});
```

### Winston (Logging)

```typescript
import winston from 'winston';

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.json(),
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' }),
  ],
});

if (process.env.NODE_ENV !== 'production') {
  logger.add(new winston.transports.Console({
    format: winston.format.simple(),
  }));
}

export default logger;
```

### DataDog (APM)

```typescript
import tracer from 'dd-trace';

tracer.init();
tracer.use('express', {
  service: 'relational-insight-api'
});
tracer.use('postgres', {
  service: 'relational-insight-db'
});
```

---

## 6. ğŸš€ DEPLOYMENT

### Docker & Docker Compose

**Dockerfile (Frontend):**
```dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .
RUN npm run build

EXPOSE 3000

CMD ["npm", "start"]
```

**Dockerfile (Backend FastAPI):**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["gunicorn", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "main:app"]
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      NEXT_PUBLIC_API_URL: http://api:8000
    depends_on:
      - api

  api:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://user:password@db:5432/relational_insight
      REDIS_URL: redis://cache:6379
    depends_on:
      - db
      - cache

  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: relational_insight
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data

  cache:
    image: redis:7-alpine

volumes:
  postgres_data:
```

---

## 7. ğŸ”§ CI/CD Pipeline

### GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Node
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run tests
        run: npm test -- --coverage
      
      - name: Build
        run: npm run build
      
      - name: Security scan
        run: npm audit --production
      
  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to production
        run: |
          # Deploy script aquÃ­
          echo "Deploying to production..."
```

---

## 8. ğŸ“ˆ PERFORMANCE OPTIMIZATION

### Frontend

```typescript
// next.config.js
const withBundleAnalyzer = require('@next/bundle-analyzer')({
  enabled: process.env.ANALYZE === 'true',
});

module.exports = withBundleAnalyzer({
  compress: true,
  productionBrowserSourceMaps: false,
  
  // Image optimization
  images: {
    formats: ['image/avif', 'image/webp'],
    deviceSizes: [640, 750, 828, 1080, 1200, 1920],
  },
  
  // Script optimization
  scripts: {
    strategy: 'lazyOnload',
  },
});
```

### Backend

```python
# FastAPI caching
from fastapi_cache2 import FastAPICache2
from fastapi_cache2.backends.redis import RedisBackend
from fastapi_cache2.decorators import cache

@app.get("/api/correlations/{analysis_id}")
@cache(expire=3600)
async def get_analysis(analysis_id: int):
    return await db.get_analysis(analysis_id)
```

---

## 9. ğŸ“‹ TECH STACK SUMMARY

```
FRONTEND:
â”œâ”€ Next.js 15
â”œâ”€ React 18
â”œâ”€ TypeScript 5
â”œâ”€ Tailwind CSS
â”œâ”€ Recharts (visualizaciÃ³n)
â””â”€ SWR (data fetching)

BACKEND (OPCIÃ“N A):
â”œâ”€ FastAPI
â”œâ”€ PostgreSQL
â”œâ”€ Redis
â”œâ”€ SQLAlchemy
â”œâ”€ Pydantic
â””â”€ JWT

BACKEND (OPCIÃ“N B):
â”œâ”€ Express.js
â”œâ”€ PostgreSQL
â”œâ”€ Redis
â”œâ”€ TypeORM
â””â”€ JWT

DEVOPS:
â”œâ”€ Docker
â”œâ”€ Docker Compose
â”œâ”€ GitHub Actions
â””â”€ Nginx (reverse proxy)

MONITORING:
â”œâ”€ Sentry (errors)
â”œâ”€ Winston (logs)
â”œâ”€ DataDog (APM)
â””â”€ Prometheus (metrics)

TESTING:
â”œâ”€ Jest
â”œâ”€ React Testing Library
â”œâ”€ Pytest/Unittest
â”œâ”€ Cypress (E2E)
â””â”€ K6 (load testing)
```

---

## ğŸ“… TIMELINE DE IMPLEMENTACIÃ“N

```
SEMANA 1: Backend Setup
â”œâ”€ Database schema
â”œâ”€ API endpoints bÃ¡sicos
â”œâ”€ AutenticaciÃ³n JWT
â””â”€ Rate limiting

SEMANA 2: IntegraciÃ³n
â”œâ”€ Frontend â†’ Backend
â”œâ”€ Persistencia de anÃ¡lisis
â”œâ”€ HistÃ³rico de usuario
â””â”€ Refactoring

SEMANA 3: Seguridad & Tests
â”œâ”€ Security audit
â”œâ”€ Tests backend
â”œâ”€ Tests integraciÃ³n
â””â”€ Performance tuning

SEMANA 4: Deployment
â”œâ”€ Docker setup
â”œâ”€ CI/CD pipeline
â”œâ”€ Staging environment
â””â”€ Go live
```

---

## âœ… CHECKLIST

```
BACKEND:
â˜ API endpoints definidos
â˜ AutenticaciÃ³n implementada
â˜ Database schema creado
â˜ Rate limiting activo
â˜ Validaciones en servidor
â˜ Tests backend (80%+ cobertura)
â˜ Error handling robusto
â˜ Logging centralizado

OPERACIONAL:
â˜ Docker images buildean
â˜ Docker Compose funciona
â˜ Environment variables .env
â˜ Database migrations working
â˜ Backups automatizados

CI/CD:
â˜ GitHub Actions workflows
â˜ Tests run on push
â˜ Build automatizado
â˜ Deployment workflow

MONITORING:
â˜ Sentry configured
â˜ Winston logger working
â˜ APM tracking
â˜ Alertas setup

SECURITY:
â˜ HTTPS configurado
â˜ CORS correcto
â˜ Headers de seguridad
â˜ Rate limiting activo
â˜ Input validation
â˜ SQL injection protection
```

---

**Con esta arquitectura tendrÃ¡s una aplicaciÃ³n lista para producciÃ³n, escalable y segura.** ğŸš€

